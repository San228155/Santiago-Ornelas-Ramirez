{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9643c2b1-d814-43d3-a159-258883945688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\"spark.databricks.sql.dsv2.unique.enabled\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af5b9a0a-8348-466c-b4a0-99fe2c38a6ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "transformation\n",
    "- table\n",
    "  - source\n",
    "  - default\n",
    "  - keys\n",
    "  - columns\n",
    "    - transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb05e48-08c7-49fc-b3e1-ed664fb85db1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import DataFrame\n",
    "sys.path.append('/Workspace/databricks_bootcamp/silver/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c0a49d-908f-4a3f-a8a4-b010e9539f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Any\n",
    "from pyspark.sql.types import IntegerType\n",
    "from functools import reduce\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import xxhash64\n",
    "from delta.tables import DeltaTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45f023c0-924b-4e3c-b1a2-49e5282526f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import meta_driven_transformations\n",
    "# importlib.reload(meta_driven_transformations)\n",
    "\n",
    "TRANSFORMATION = meta_driven_transformations.TRANSFORMATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33fcba3-2c66-4222-b3c4-99a9c601e8e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def default_transformations(df: DataFrame, df_name: str, meta_data: dict[str, Any]) -> DataFrame:\n",
    "    table_configs = meta_data[df_name]\n",
    "    if \"defaults\" in table_configs:\n",
    "        df = df.select(*[\n",
    "            F.lower(F.trim(F.col(c))).alias(c) for c in df.columns\n",
    "        ])\n",
    "\n",
    "    return df\n",
    "\n",
    "    # we lower and upper every column, even customer ids becasue we do not want mixed upper and lower ids, and using upper is just as arbitrary as lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ea54ac-4d06-44ce-a3e6-84fdbe42ad0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns(df: DataFrame, df_name: str, meta_data: dict[str, Any]) -> DataFrame:\n",
    "    table_configs = meta_data[df_name]\n",
    "    columns = table_configs[\"columns\"]\n",
    "\n",
    "    select_expr = [F.col(col_val[\"source\"]).alias(col_key) for col_key, col_val in columns.items()]\n",
    "    \n",
    "    return df.select(\n",
    "            *select_expr\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a75625-7017-4466-98fa-e3bf04abf79b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_nulls_and_empty_strings(df: DataFrame, df_name: str, meta_data: dict[str, Any]) -> DataFrame:\n",
    "    table_configs = meta_data[f\"{df_name}\"]\n",
    "    string_cols: str = []\n",
    "    int_cols: str = []\n",
    "    date_cols: dict[str, list[str]] = {}\n",
    "\n",
    "    for col_key, col_val in table_configs[\"columns\"].items():\n",
    "        cast_types = col_val.get(\"cast\")\n",
    "        if \"cast\" not in col_val:\n",
    "            string_cols.append(col_key)\n",
    "        elif \"int\" in cast_types:\n",
    "            int_cols.append(col_key)\n",
    "        elif \"date\" in cast_types:\n",
    "            date_format = cast_types[\"default_value\"]\n",
    "            date_cols.setdefault(date_format, []).append(col_key)\n",
    "        else:\n",
    "            raise ValueError(f\"Type {cast_types} not supported for column {col_key}\")\n",
    "\n",
    "    expr = []\n",
    "\n",
    "    [\n",
    "        expr.append(\n",
    "            F.when(\n",
    "                (F.col(c).isNull()) | (F.col(c) == \"\"), \"unknown\"\n",
    "                )\\\n",
    "                .otherwise(F.col(c))\\\n",
    "                .alias(c)\n",
    "            )\n",
    "        for c in string_cols\n",
    "    ]\n",
    "\n",
    "    [\n",
    "        expr.append(\n",
    "            F.when(\n",
    "                (F.col(c).isNull()) | (F.col(c) == \"\"), \"0\"\n",
    "                )\\\n",
    "                .otherwise(F.col(c))\\\n",
    "                .alias(c)\n",
    "            )\n",
    "        for c in int_cols\n",
    "    ]\n",
    "\n",
    "    for date_format, c in date_cols.items():\n",
    "        for col in c:\n",
    "            expr.append(\n",
    "            F.when(\n",
    "                (F.col(col).isNull()) | (F.col(col) == \"\"), date_format\n",
    "                )\\\n",
    "                .otherwise(F.col(col))\\\n",
    "                .alias(col)\n",
    "            )\n",
    "\n",
    "    return df.select(\n",
    "        *expr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "247b1897-ff9f-4c54-aab4-238478cfa122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def apply_enum_aliases(df: DataFrame, df_name: str, meta_data: dict[str, any]) -> DataFrame:\n",
    "    table_configs = meta_data[df_name]\n",
    "\n",
    "    exprs = []\n",
    "    quarantine_dfs = []\n",
    "\n",
    "    for col_key, col_val in table_configs[\"columns\"].items():\n",
    "        if \"map\" in col_val:\n",
    "            w = None\n",
    "            match_cond = None\n",
    "\n",
    "            for desired_val, possible_vals in col_val[\"map\"].items():\n",
    "                cond = F.col(col_key).isin(possible_vals)\n",
    "                match_cond = cond if match_cond is None else (match_cond | cond)\n",
    "                w = F.when(cond, F.lit(desired_val)) if w is None else w.when(cond, F.lit(desired_val))\n",
    "\n",
    "            normalized_col = w.otherwise(F.lit(\"unknown\"))\n",
    "\n",
    "        else:\n",
    "            normalized_col = F.col(col_key)\n",
    "            match_cond = None \n",
    "\n",
    "        exprs.append(normalized_col.alias(col_key))\n",
    "\n",
    "        if \"validation\" in col_val and \"enum\" in col_val[\"validation\"]:\n",
    "            valid_values = col_val[\"validation\"][\"enum\"]\n",
    "\n",
    "            invalid_condition = (\n",
    "                ~normalized_col.isin(valid_values)\n",
    "            )\n",
    "\n",
    "            invalid_rows = (\n",
    "                df.filter(invalid_condition)\n",
    "                  .select(\n",
    "                      *[\n",
    "                          normalized_col.alias(col_key)\n",
    "                          if c == col_key\n",
    "                          else F.lit(\"unknown\").alias(c)\n",
    "                          for c in df.columns\n",
    "                      ]\n",
    "                  )\n",
    "            )\n",
    "\n",
    "            quarantine_dfs.append(invalid_rows)\n",
    "\n",
    "    main_df = df.select(*exprs)\n",
    "\n",
    "    if quarantine_dfs:\n",
    "        quarantine_df = reduce(DataFrame.unionByName, quarantine_dfs)\n",
    "        # return main_df.unionByName(quarantine_df)\n",
    "\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992ba896-48ad-4f93-a2fd-f7a0c4e91420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def casting(df: DataFrame, df_name: str, meta_data:dict[str, Any]) -> DataFrame:\n",
    "    table_configs = meta_data[f\"{df_name}\"]\n",
    "    expr = []\n",
    "\n",
    "    for col_key, col_val in table_configs[\"columns\"].items():\n",
    "        \n",
    "        if not col_val.get(\"cast\"):\n",
    "            expr.append(F.col(col_key))\n",
    "        \n",
    "        elif \"int\" in col_val[\"cast\"]:\n",
    "            expr.append(F.col(col_key).cast(\"int\").alias(col_key))\n",
    "        \n",
    "        elif \"date\" in col_val[\"cast\"]:\n",
    "            expr.append(F.try_to_date(F.col(col_key), col_val[\"cast\"][\"date\"]).alias(col_key))\n",
    "\n",
    "    return df.select(\n",
    "        *expr\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9066984e-7395-4f7b-857f-4b13f8a59d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def handle_hyphon(df: DataFrame, df_name: str, meta_data: dict[str, Any]) -> DataFrame:\n",
    "    table_configs = meta_data[df_name]\n",
    "    expr = []\n",
    "\n",
    "    for col_key, col_val in table_configs[\"columns\"].items():\n",
    "        if col_val.get(\"replace_hyphon\", False):\n",
    "            expr.append(\n",
    "                F.regexp_replace(\n",
    "                    F.col(col_key),\n",
    "                    r\"[\\s\\-]+\",\n",
    "                    \"_\"\n",
    "                ).alias(col_key)\n",
    "            )\n",
    "        elif col_val.get(\"remove_hyphon\"):\n",
    "            expr.append(\n",
    "                F.regexp_replace(F.col(col_key), \"-\", \"\").alias(col_key)\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            expr.append(F.col(col_key))\n",
    "    return df.select(*expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1824b7-e9d9-4c6d-b87c-5adf487e1b94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# table = \"silver_crm_sales_info\"\n",
    "# df = default_transformations(df, table,TRANSFORMATION) \n",
    "# df = rename_columns(df, table, TRANSFORMATION)\n",
    "# df = handle_nulls_and_empty_strings(df, table, TRANSFORMATION)\n",
    "# df = apply_enum_aliases(df, table, TRANSFORMATION)\n",
    "# df = casting(df, table, TRANSFORMATION)\n",
    "# df = replace_blank_spaces(df, table, TRANSFORMATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eae6dec-23a6-40af-b0b0-c2d0b5b4215a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_augmentation(df, df_name, meta_data):\n",
    "    table = meta_data[df_name]\n",
    "    expr_list = []\n",
    "    if augmented_cols_list := table.get(\"data_augmentation_columns\", []):\n",
    "        ops = {\n",
    "            \"+\": lambda a, b: a + b,\n",
    "            \"-\": lambda a, b: a - b,\n",
    "            \"*\": lambda a, b: a * b,\n",
    "            \"/\": lambda a, b: a / b,\n",
    "            }\n",
    "\n",
    "        for cols in df.columns:\n",
    "            if cols in augmented_cols_list:\n",
    "                operation = table.get(\"columns\").get(cols).get(\"expression\")\n",
    "\n",
    "                left  = F.col(operation[0])\n",
    "                right = F.col(operation[2])\n",
    "                op = operation[1]\n",
    "\n",
    "                expr = F.when((left == 0) | (right == 0), F.col(cols))\\\n",
    "                    .otherwise(F.round(ops[op](left, right))).cast(\"int\")\n",
    "                \n",
    "                expr_list.append(\n",
    "                    F.when(\n",
    "                        F.col(cols) == 0,\n",
    "                        expr\n",
    "                    ).otherwise(F.col(cols))\\\n",
    "                    .alias(cols)\n",
    "                )\n",
    "            else:\n",
    "                expr_list.append(F.col(cols))\n",
    "        return df.select(*expr_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae7cc0f-1618-499e-b31b-6dbe1c9c76cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def intelligent_key_preparation(preprocessed_tables, meta_data):\n",
    "    intelligent_tables = {}\n",
    "    for df_name, df in preprocessed_tables.items():\n",
    "        table = meta_data[df_name]\n",
    "        if table.get(\"intelligent_key\"):\n",
    "            expr_list = []\n",
    "\n",
    "            intelligent_key_mapping = table.get(\"intelligent_key\", {})\n",
    "\n",
    "            derived_cols = intelligent_key_mapping.get(\"intelligent_key\")[\"source\"]\n",
    "\n",
    "            untouched_cols = [\n",
    "                F.col(col)\n",
    "                for col in df.columns\n",
    "                if col != derived_cols\n",
    "            ]\n",
    "\n",
    "            for new_column_name, column_values in intelligent_key_mapping.items():\n",
    "                source_column = column_values[\"source\"]\n",
    "                start, length = column_values[\"substr\"]\n",
    "\n",
    "                expr_list.append(\n",
    "                        F.substring(F.col(source_column), start, length).alias(new_column_name)\n",
    "                )\n",
    "\n",
    "            expr_list.extend(untouched_cols)\n",
    "            \n",
    "            int_key_removed_df = df.select(*expr_list)\n",
    "\n",
    "            intelligent_tables [df_name] = int_key_removed_df\n",
    "        else: \n",
    "            intelligent_tables[df_name] = df\n",
    "    return intelligent_tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89f3401-93ad-4c83-8880-b5b041c27efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def surrogate_key_addition(intelligent_tables, meta_data):\n",
    "    surrogate_key_dict = {}\n",
    "    surrogate_tables = {}\n",
    "\n",
    "    for table_name in intelligent_tables:\n",
    "        surrogate_config = list(meta_data[table_name][\"surrogate_key\"].keys())\n",
    "        if \"dim_customer_sk\" in surrogate_config:\n",
    "            surrogate_key_dict.setdefault(\"dim_customer_sk\", []).append(table_name)\n",
    "        \n",
    "        if \"dim_product_sk\" in surrogate_config:\n",
    "            surrogate_key_dict.setdefault(\"dim_product_sk\", []).append(table_name)\n",
    "\n",
    "        if \"dim_category_sk\" in surrogate_config:\n",
    "            surrogate_key_dict.setdefault(\"dim_category_sk\", []).append(table_name)\n",
    "\n",
    "    for key, tables in surrogate_key_dict.items():\n",
    "\n",
    "        # drop_logic = f\"DROP TABLE IF EXISTS bike_data_lakehouse.surrogate_keys.surrogate_key_{key}_table\"\n",
    "\n",
    "        # spark.sql(drop_logic)\n",
    "        column_name = meta_data[tables[0]][\"surrogate_key\"][key]\n",
    "\n",
    "        create_table_ddl = f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS bike_data_lakehouse.surrogate_keys.surrogate_key_{key}_table (\n",
    "            {column_name} STRING NOT NULL,\n",
    "            {key} LONG NOT NULL\n",
    "        )\n",
    "        \"\"\"\n",
    "        spark.sql(create_table_ddl)\n",
    "\n",
    "        dfs = [\n",
    "            intelligent_tables[t].select(column_name)\n",
    "            for t in tables\n",
    "        ]\n",
    "\n",
    "        dimension_df = reduce(\n",
    "            lambda df1, df2: df1.union(df2),\n",
    "            dfs\n",
    "        )\n",
    "\n",
    "        dimension_df = (\n",
    "            dimension_df\n",
    "            .select(F.col(column_name))\n",
    "            .distinct()\n",
    "        )\n",
    "\n",
    "\n",
    "        df_with_sk = dimension_df.withColumn(\n",
    "            key,\n",
    "            xxhash64(F.col(column_name))\n",
    "        )\n",
    "\n",
    "        delta_target = DeltaTable.forName(spark, f\"bike_data_lakehouse.surrogate_keys.surrogate_key_{key}_table\")\n",
    "\n",
    "        (\n",
    "        delta_target.alias(\"t\")\n",
    "        .merge(\n",
    "            df_with_sk.alias(\"s\"),\n",
    "            f\"t.{column_name} = s.{column_name}\"\n",
    "        )\n",
    "        .whenNotMatchedInsertAll()\n",
    "        .execute()\n",
    "        )\n",
    "    \n",
    "    sk_dict = {}\n",
    "    sk_tables_sql = \"SHOW TABLES IN bike_data_lakehouse.surrogate_keys\"\n",
    "    sk_tables = [row.tableName for row in spark.sql(sk_tables_sql).collect()]\n",
    "    \n",
    "    for table in sk_tables:\n",
    "        sk_dict[table] = spark.read.table(f\"bike_data_lakehouse.surrogate_keys.{table}\")\n",
    "\n",
    "    for table_name, data_frame in intelligent_tables.items():\n",
    "        for surrogate_key, column_name in meta_data[table_name][\"surrogate_key\"].items():\n",
    "            sk_join_table = sk_dict[f\"surrogate_key_{surrogate_key}_table\"]\n",
    "            data_frame = data_frame.join(\n",
    "                F.broadcast(sk_join_table),\n",
    "                on=[column_name],\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "        surrogate_tables[f\"{table_name}\"] = data_frame # we do not do the joins, we just overwrite it \n",
    "    return surrogate_tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37feadbf-3f3d-4dce-bac3-a09632e34635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def second_nf_configuration(surrogate_tables, meta_data):\n",
    "    # only designed for crm_customer_info\n",
    "    normalized_tables = {}\n",
    "\n",
    "    for df_name, df in surrogate_tables.items():\n",
    "        table = meta_data[df_name]\n",
    "        primary_key = list(table[\"primary_key\"].keys())\n",
    "\n",
    "        \n",
    "        pk_unknown_conditions = [\n",
    "            F.col(col) == F.lit(\"unknown\") for col in primary_key\n",
    "        ]\n",
    "\n",
    "        unknown_condition = reduce(lambda a, b: a | b, pk_unknown_conditions)\n",
    "\n",
    "        malformed_condition = (F.col(\"duplicate_rows\") > 1) | unknown_condition\n",
    "\n",
    "        duplicate_rows = (\n",
    "            df.groupBy(*primary_key)\n",
    "            .agg(F.count(\"*\").alias(\"duplicate_rows\"))\n",
    "            .filter(malformed_condition)\n",
    "        )\n",
    "\n",
    "        duplicate_keys = [\n",
    "            tuple(row[col] for col in primary_key)\n",
    "            for row in duplicate_rows.collect()\n",
    "        ]\n",
    "\n",
    "        duplicate_key_structs = [\n",
    "            F.struct(*[F.lit(v) for v in key_tuple])\n",
    "            for key_tuple in duplicate_keys\n",
    "        ]\n",
    "\n",
    "\n",
    "        # quarantine_df = df.join(duplicate_rows, on=primary_key, how=\"inner\")\n",
    "\n",
    "        second_nf_compliant_rows = df.filter(\n",
    "            ~F.struct(*primary_key).isin(duplicate_key_structs)\n",
    "        )\n",
    "\n",
    "        # upload to a quarantine\n",
    "\n",
    "        normalized_tables[df_name] = second_nf_compliant_rows\n",
    "    \n",
    "    return normalized_tables \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b60537e-d457-45a4-a231-0c3658d4a06b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_table(normalized_tables, meta_data):\n",
    "    catalog = \"bike_data_lakehouse\"\n",
    "    schema = \"silver\"\n",
    "    for df_name, df in normalized_tables.items():\n",
    "\n",
    "        drop_logic = f\"DROP TABLE IF EXISTS bike_data_lakehouse.silver.{df_name}\"\n",
    "\n",
    "        spark.sql(drop_logic)\n",
    "\n",
    "        table_meta = meta_data[df_name]\n",
    "        primary_key = table_meta[\"primary_key\"]\n",
    "\n",
    "        cast_types = {\"date\", \"int\"}\n",
    "\n",
    "        expr = []\n",
    "        table_constraints = []\n",
    "        composite_pk_cols = []\n",
    "\n",
    "        is_composite_pk = len(primary_key) > 1\n",
    "\n",
    "        for col_name, col_val in table_meta[\"columns\"].items():\n",
    "\n",
    "            cast_info = col_val.get(\"cast\", {})\n",
    "            cast_type = next((k for k in cast_info if k in cast_types), None)\n",
    "            dtype = cast_type.upper() if cast_type else \"STRING\"\n",
    "\n",
    "            if col_name in primary_key:\n",
    "                length_of_pk = primary_key[col_name]\n",
    "                if is_composite_pk:\n",
    "                    expr.append(\n",
    "                        f\"{col_name} {dtype} NOT NULL \"\n",
    "                        f\"CHECK (LENGTH({col_name}) = {length_of_pk})\"\n",
    "                    )\n",
    "                    composite_pk_cols.append(col_name)\n",
    "                else:\n",
    "                    expr.append(\n",
    "                        f\"{col_name} {dtype} NOT NULL PRIMARY KEY UNIQUE \"\n",
    "                        f\"CHECK (LENGTH({col_name}) = {length_of_pk})\"\n",
    "                    )\n",
    "            else:\n",
    "                expr.append(f\"{col_name} {dtype}\")\n",
    "\n",
    "        if is_composite_pk:\n",
    "            cols = \", \".join(composite_pk_cols)\n",
    "            table_constraints.append(\n",
    "                f\"CONSTRAINT pk_{df_name} PRIMARY KEY ({cols})\"\n",
    "            )\n",
    "            table_constraints.append(\n",
    "                f\"CONSTRAINT uq_{df_name} UNIQUE ({cols})\"\n",
    "            )\n",
    "\n",
    "        column_expr = \", \".join(expr + table_constraints)\n",
    "\n",
    "        create_table_sql = f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {catalog}.{schema}.{df_name} (\n",
    "                {column_expr}\n",
    "            )\n",
    "        \"\"\"\n",
    "\n",
    "        show_tables = f\"SHOW TABLES IN {catalog}.{schema}\"\n",
    "        existing_tables = spark.sql(show_tables).collect()\n",
    "        existing_table_names = {row.tableName for row in existing_tables}\n",
    "\n",
    "        if df_name not in existing_table_names:\n",
    "            df.write.mode(\"append\").saveAsTable(f\"{catalog}.{schema}.{df_name}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"ERRROR - failed to write: this table {df_name} already exists in schema\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67f4663d-c717-4ac3-9635-729254f688b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import meta_driven_transformations\n",
    "importlib.reload(meta_driven_transformations)\n",
    "\n",
    "TRANSFORMATION = meta_driven_transformations.TRANSFORMATION\n",
    "# spark.conf.set(\"spark.sql.storeAssignmentPolicy\", \"ANSI\")\n",
    "\n",
    "def clean_data_tables(TRANSFORMATION:dict[str, Any]):\n",
    "    print(\"cleaning_data\")\n",
    "    incoming_schema = \"bike_data_lakehouse.bronze.\"\n",
    "    table_objects = spark.sql(\"SHOW TABLES IN bike_data_lakehouse.bronze\")\n",
    "    table_list = [tables.tableName for tables in table_objects.collect()]\n",
    "    clean_data = {}\n",
    "\n",
    "    for table in table_list:\n",
    "        print(table)\n",
    "        df = spark.read.table(f\"{incoming_schema}{table}\")\n",
    "        silver_table_key = f\"silver_{table}\"\n",
    "        df = default_transformations(df, silver_table_key, TRANSFORMATION) \n",
    "        df = rename_columns(df, silver_table_key, TRANSFORMATION)\n",
    "        df = handle_nulls_and_empty_strings(df, silver_table_key, TRANSFORMATION)\n",
    "        df = handle_hyphon(df, silver_table_key, TRANSFORMATION)\n",
    "        df = casting(df, silver_table_key, TRANSFORMATION)\n",
    "        clean_data[silver_table_key] = df\n",
    "    \n",
    "    return clean_data\n",
    "        \n",
    "clean_data = clean_data_tables(TRANSFORMATION)\n",
    "\n",
    "def preprocess_data_tables(TRANSFORMATION:dict[str, Any], clean_data) -> dict[str, Any]:\n",
    "    print(\"preprocessing data\")\n",
    "    preprocessed_tables = {}\n",
    "    incoming_schema = \"bike_data_lakehouse.bronze.\"\n",
    "    \n",
    "    for table_name, df in clean_data.items():\n",
    "        print(table_name)\n",
    "        df = apply_enum_aliases(df, table_name, TRANSFORMATION)\n",
    "        df = data_augmentation(df, table_name, TRANSFORMATION)\n",
    "\n",
    "        preprocessed_tables[table_name] = df\n",
    "    return preprocessed_tables\n",
    "\n",
    "preprocessed_tables = preprocess_data_tables(TRANSFORMATION, clean_data)\n",
    "\n",
    "def silver_table_upload(TRANSFORMATION:dict[str, Any], preprocessed_tables) -> None:\n",
    "    print(\"uploading\")\n",
    "    intelligent_tables = intelligent_key_preparation(preprocessed_tables, TRANSFORMATION)\n",
    "    surrogate_tables = surrogate_key_addition(intelligent_tables, TRANSFORMATION)\n",
    "    normalized_tables = second_nf_configuration(surrogate_tables, TRANSFORMATION) # also deduplication\n",
    "    create_table(normalized_tables, TRANSFORMATION)\n",
    "\n",
    "silver_table_upload(TRANSFORMATION, preprocessed_tables)\n",
    "# 1. Column Standardization (Rename Columns)\n",
    "# 2. Default Transformations (basic string cleanup)\n",
    "# 3. Structural Cleaning (trim, whitespace normalization, hyphen handling)\n",
    "# 4. Null & Empty Handling\n",
    "# 5. Type Casting\n",
    "# 6. Enum / Domain Mapping\n",
    "# 7. Data Validation & Constraint Enforcement, no??? jsut save the data \n",
    "# 8. Business Key Identification\n",
    "# 9. Intelligent Key Preparation\n",
    "# 10. Surrogate Key Generation\n",
    "# 11. Data Augmentation (derived columns)\n",
    "# 12. 2NF Normalization\n",
    "# 13. Final Quality Checks\n",
    "# 14. Table Creation (Silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870abada-581d-4d6d-b3e6-d8d8653740a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bike_data_lakehouse.silver.silver_crm_product_info\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82443195-efad-48fa-aa3d-4eaac24df9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "clean_tables{table name: dataframe}  \n",
    "surrogate_key_dict{new_column_name, table_name}  \n",
    "column name [natural key]  \n",
    "dfs = df with natural key\n",
    "bunch of unions  \n",
    "surrogate_tables {surrogate_key_name: unioned_data_Frame}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28cbc72-76a7-480d-a95a-697dc4d43e8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "grab a table  \n",
    "- what surrogate keys does it have\n",
    "join the original one with the corresponding surrogate key table, possible > 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4d081ce-c436-4792-bf74-a3599c56e7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for table_name, data_frame in clean_tables.items():\n",
    "#     for surrogate_key, column_name in meta_data[table_name][\"surrogate_key\"].items():\n",
    "#         path = f\"bike_data_lakehouse.surrogate_keys.surrogate_key_{surrogate_key}_table\"\n",
    "#         sk_table = spark.read.table(path) # could optimzie to only read the table once\n",
    "#         result = data_frame.join(\n",
    "#             F.broadcast(sk_table),\n",
    "#             on=[column_name],\n",
    "#             how=\"left\"\n",
    "#         )\n",
    "\n",
    "#         # sk_table = spark.read.table(\"surrogate_keys.surrogate_key_{key}_table\")\n",
    "#         print(surrogate_keys)\n",
    "#         # data_frame.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84554187-10b5-40ff-b67f-70111c6051ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "column_name = meta_data[table][\"surrogate_key\"][key]  \n",
    "df1.join(df2, on = column_name, how = \"outer\").join(df3, on = column_name, how = \"outer\").select(column_name.alias(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "144fd9d5-8311-42d8-9c86-6197823ada7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "we look at all the tables\n",
    "if it has a surrogate then we join all the tables that have this surrogate key condition on the value of the surrogate key.items()\n",
    "we then make a hash based on the id we chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37148708-fe9a-4523-9593-c30fe577ebcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# rewrite the entire data frames... by using joins...\n",
    "# use decorators (logging and timing)\n",
    "# make hard writes to the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bbdff75-44d7-409e-92d9-9227b29f5999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bike_data_lakehouse.silver.silver_crm_sales_info\n",
    "WHERE LENGTH(dim_sales_customer_id) <> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1300594-eb9d-4b87-8515-962a6464929d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "spark.conf.set(\"spark.sql.storeAssignmentPolicy\", \"ANSI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff9e0237-3b1f-4816-9ae2-7793c67dee2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = \"a\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS blah (\n",
    "              blah type\n",
    "          )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452b9884-ce37-476f-9314-f52a2e58e349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "['so61548', 'so61560', 'so61561', 'so61562', 'so61636', 'so67551', 'so68588', 'so68745', 'so68745', 'so68889']  \n",
    "['ca_1098', 'hl_u509', 'cl_9009', 'fe_6654', 'bc_m005', 'bc_m005', 'pk_7098', 'hl_u509', 'gl_h102_l', 'tt_r982']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2e86c3a-7b12-4ac1-b31b-9634990c9f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bike_data_lakehouse.silver.silver_crm_sales_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ec5ae9-a5f8-4b58-adce-681ee7710d9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "select \n",
    "  \"pk\".alias(\"int key\")\n",
    "    separated keys\n",
    "  other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f1d282-8fd1-4442-bb30-59e8dd290168",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770573728232}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bike_data_lakehouse.bronze.crm_sales_info\n",
    "WHERE sls_prd_key LIKE \"%RA-H123%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c0d0ba-7ec0-4c85-8191-28e943b78bb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM bike_data_lakehouse.bronze.erp_product_category_g1_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d386ac1d-1609-4440-8ed2-3dc74d487986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# separate intelligent keys into whatever"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5384475969939531,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
