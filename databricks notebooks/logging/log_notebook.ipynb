{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc44d8a-4a1e-43ef-82d1-042bbf935de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# LOGGING - DEFINE logging functions\n",
    "# Purpose: Define pipe line and data quality logs and metric logs\n",
    "# Input: **kwargs â€“ dynamic parameters passed to logging functions\n",
    "# Output: (DELTA TABLE)\n",
    "## - data quality tests\n",
    "## - pipe line runs\n",
    "## - metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37f3c151-5899-4b29-a2b7-3ce9d1e16ca7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CONFIG/PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae3507e4-dffc-4c62-aa6d-87ba2b98e0c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63bbc16-aa90-41cb-8869-c5dd081cba0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "below is used for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1de583-d974-4bd4-849d-751f73dcf7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 12
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "USE CATALOG harris_county_catalog\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6be0226-ebd1-410a-94ed-10384f0193a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"year\", \"\")\n",
    "year = dbutils.widgets.get(\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4201d8dc-c20c-4384-80f7-53eaf966efc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.empty-table+json": {
       "directive_name": "DropTable"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# DROP TABLE IF EXISTS logs.log_pipe_line_runs;\n",
    "# DROP TABLE IF EXISTS logs.log_data_quality_tests;\n",
    "# DROP TABLE IF EXISTS logs.log_table_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4d24b18-c62f-46ee-8ab3-6ba69d69087a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## TABLE CREATION AND SCHEMA DEFINITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "362bf4ff-3e1f-4d46-a7eb-a7a5e13f76ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" -- name should be changed to log not logs for table name\n",
    "        CREATE TABLE IF NOT EXISTS logs.log_pipe_line_runs (\n",
    "            run_id STRING\n",
    "            , notebook STRING\n",
    "            , stage STRING\n",
    "            , input_table STRING\n",
    "            , output_table STRING\n",
    "            , status STRING\n",
    "            , error_message STRING\n",
    "            , run_time DOUBLE\n",
    "        )\n",
    "        \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS logs.log_data_quality_tests (\n",
    "            run_id STRING\n",
    "            , notebook STRING\n",
    "            , test_table STRING\n",
    "            , test_name STRING\n",
    "            , status STRING\n",
    "            , error_message STRING\n",
    "        )   \n",
    "    \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS logs.log_table_metrics(\n",
    "            run_id STRING\n",
    "            , notebook STRING\n",
    "            , stage STRING\n",
    "            , metric_name STRING\n",
    "            , metric_value DOUBLE\n",
    "        )\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f868b3f8-273f-412c-a8d9-d655dffc3b06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eec5695-5e68-439e-b2ce-bf552e7d10ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### PIPELINE RUNS LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ce78b4-64f8-4b81-8a35-2c9b1cf2d70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_pipeline_runs(notebook:str, stage:str, input_table:str, output_table:str, status:str, error_message:str=None, run_time:float=0):\n",
    "    # job_run_id = spark.conf.get(\"spark.databricks.job.runId\", None)\n",
    "    \n",
    "    row = Row(\n",
    "        run_id = year\n",
    "        # run_id = job_run_id\n",
    "        , notebook = notebook\n",
    "        , stage = stage\n",
    "        , input_table = input_table\n",
    "        , output_table = output_table\n",
    "        , status = status\n",
    "        , error_message = error_message or \"\"\n",
    "        , run_time = run_time\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame([row])   \n",
    "    df = df.withColumn(\"run_time\", F.col(\"run_time\").cast(\"double\"))\n",
    "    df.createOrReplaceTempView(\"new_row\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO logs.log_pipe_line_runs as t\n",
    "        USING new_row as s\n",
    "        ON t.run_id = s.run_id\n",
    "        AND t.notebook = s.notebook\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        status = s.status\n",
    "        , error_message = s.error_message\n",
    "        , run_time = s.run_time\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f8edf88-17ff-436d-88b2-fa7809dacf7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DATA QUALITY TESTS LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1cf9b83-211b-4351-844e-8e9f8fe9cea5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_data_quality_tests(notebook:str, test_table:str, test_name:str, status:str, error_message:str=None):\n",
    "\n",
    "    # job_run_id = spark.conf.get(\"spark.databricks.job.runId\", None)\n",
    "\n",
    "    row = Row(\n",
    "        run_id = year\n",
    "        # run_id = job_run_id\n",
    "        , notebook = notebook\n",
    "        , test_table = test_table\n",
    "        , test_name = test_name\n",
    "        , status = status\n",
    "        , error_message = error_message or \"\"\n",
    "    )\n",
    "\n",
    "    df = spark.createDataFrame([row])\n",
    "\n",
    "    df.createOrReplaceTempView(\"dq_row\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO logs.log_data_quality_tests t\n",
    "        USING dq_row s\n",
    "        ON  t.run_id      = s.run_id\n",
    "        AND t.test_table = s.test_table\n",
    "        AND t.test_name = s.test_name\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        status        = s.status,\n",
    "        error_message = s.error_message\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2b8cbe6-3468-44cb-b881-4ba8b0f12dea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### METRICS LOGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1f2e685-8cb6-49ec-afd0-d55a64cf787d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def metric_table_upload(notebook: str, stage: str, metric_name_list:int, metric_value_list:int):\n",
    "    # job_run_id = spark.conf.get(\"spark.databricks.job.runId\", None)\n",
    "\n",
    "    if len(metric_name_list) != len(metric_value_list):\n",
    "        print(\"metric name list and metric value list do not have same number of elements\")\n",
    "        raise ValueError(\"metric_name_list and metric_value_list do not have the same number of elements\")\n",
    "    \n",
    "    metric_data_frame = []\n",
    "\n",
    "    for i in range(len(metric_name_list)):\n",
    "        row = Row(\n",
    "            # run_id = job_run_id\n",
    "            run_id = year\n",
    "            , notebook = notebook\n",
    "            , stage = stage\n",
    "            , metric_name = metric_name_list[i]\n",
    "            , metric_value=round(float(metric_value_list[i]),2)\n",
    "        )\n",
    "        metric_data_frame.append(row)\n",
    "\n",
    "    df = spark.createDataFrame(metric_data_frame) \\\n",
    "            .withColumn(\"metric_value\", F.col(\"metric_value\").cast(\"double\"))\n",
    "\n",
    "    # df.printSchema()\n",
    "    \n",
    "    df.createOrReplaceTempView(\"new_rows\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO logs.log_table_metrics t\n",
    "        USING new_rows s\n",
    "        ON t.run_id = s.run_id\n",
    "        AND t.notebook = s.notebook\n",
    "        AND t.metric_name = s.metric_name\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "        metric_value = s.metric_value\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5771950607532442,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "log_notebook",
   "widgets": {
    "year": {
     "currentValue": "2025",
     "nuid": "5bc64fa6-343e-42af-828a-e977dc520317",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "year",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "year",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
