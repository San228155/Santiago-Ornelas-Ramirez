{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f683f2c9-a81f-4070-bdd5-43a93063771f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We want to use a foreloop to load the data and pass it to a delta table  \n",
    "Infer schema, header true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9a4c725d-9c30-46ef-b2ec-b6c1f305f71d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"/Volumes/bike_data_lakehouse/raw_data/source_crm/cust_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377ff45a-2540-4ec0-a3fc-1b144d902368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "declare Catalog\n",
    "write with raw_data schema  \n",
    "for loop the volume  \n",
    "for loop table  \n",
    "read table  \n",
    "save in bronze schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "768a859d-52ee-4491-a52f-c29b305e7c01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark.sql import DataFrame\n",
    "from typing import Any\n",
    "sys.path.append('/Workspace/databricks_bootcamp/bronze/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15b30115-fd07-4b7b-8886-58a229d6ce34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from meta_driven_ingestion import INGESTION\n",
    "catalog = \"bike_data_lakehouse\"\n",
    "ingestion_schema = \"raw_data\"\n",
    "output_schema = \"bronze\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f88b510b-5e29-41a8-a048-9adfa250dc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "TO DO: Create table with schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb86c80-adcd-453d-b609-00bea240fcae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def missing_configs(config_dict: dict) -> None:\n",
    "    REQUIRED_FIELDS: list[str] = [\"source\", \"table_type\", \"allow_overwrite\"]\n",
    "\n",
    "    missing_configs:list[str] = [field for field in REQUIRED_FIELDS if field not in config_dict]\n",
    "\n",
    "    if missing_configs:\n",
    "        raise ValueError(f\"Missing required metadata fields: {missing_configs}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def overwrite_allowed(original_table_name: str, config_dict: dict) -> None:\n",
    "    # Check overwrite logic is allwoed\n",
    "    if not config_dict.get(\"allow_overwrite\"):\n",
    "        raise ValueError(f\"{original_table_name} does not allow table overwrite\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def valid_file_type(original_table_name: str, config_dict: dict)-> None:\n",
    "    supported_types = {'CSV'} #has to be in upper\n",
    "    if config_dict['table_type'].upper() not in supported_types:\n",
    "        raise ValueError(f\"ERROR: Unsupported file {original_table_name}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def path_checker(path: str)-> bool:\n",
    "    # Check if path exists in \n",
    "    try:\n",
    "        dbutils.fs.ls(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "def empty_table(row_count: int, original_table_name: str)-> None:\n",
    "    # Check if table is empty\n",
    "    if row_count < 1:\n",
    "        raise ValueError(f\"Data frame is empty: {original_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b093d61d-1f32-49c5-8337-b6a9d24ba36d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingestion(meta_data:dict[str, Any]) -> None:\n",
    "\n",
    "    for volume, table_dicts in meta_data.items():\n",
    "        for clean_table_name, config_dict in table_dicts.items():\n",
    "            # Metadata validation\n",
    "            missing_configs(config_dict= config_dict)\n",
    "\n",
    "            original_table_name: str = config_dict[\"source\"]\n",
    "\n",
    "            # Check overwrite logic is allwoed\n",
    "            overwrite_allowed(original_table_name=original_table_name, config_dict=config_dict)\n",
    "            \n",
    "            # Check table for valid file type\n",
    "            valid_file_type(original_table_name=original_table_name, config_dict=config_dict)\n",
    "\n",
    "            print(f\"Ingesting {original_table_name}\")\n",
    "\n",
    "            reader = spark.read.option(\"header\", \"true\")\n",
    "            path: str = f\"/Volumes/{catalog}/{ingestion_schema}/{volume}/{original_table_name}.csv\"\n",
    "            \n",
    "            # Check if path exists\n",
    "            if not path_checker(path):\n",
    "                raise ValueError(f\"File not found: {path}\")\n",
    "            \n",
    "            df:DataFrame = reader.csv(path)\n",
    "\n",
    "            row_count:int = df.count()\n",
    "\n",
    "            # Table is empty\n",
    "            empty_table(row_count=row_count, original_table_name=original_table_name)\n",
    "            \n",
    "            print(f\"number of rows in {clean_table_name}: {row_count}\")\n",
    "\n",
    "            data_frame_columns = df.columns\n",
    "            print(f\"columns of table {original_table_name}: {data_frame_columns}\")\n",
    "\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"{catalog}.{output_schema}.{clean_table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e653a7-6319-40a6-b60c-b6b6e0a71c9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion(meta_data=INGESTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13cc8aba-b031-4ef1-8f3f-9a91cbb7ef44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "{'source_crm': {'customer_info': {'source': 'cust_info', 'table_type': 'CSV'}, 'product_info': {'source': 'prd_info', 'table_type': 'CSV'}, 'sales_info': {'source': 'sales_details', 'table_type': 'CSV'}}, 'source_erp': {'Customer_AZ_12': {'source': 'CUST_AZ12', 'table_type': 'CSV'}, 'Location_A101': {'source': 'LOC_A101', 'table_type': 'CSV'}, 'Product_Category_G1_V2': {'source': 'PX_CAT_G1V2', 'table_type': 'CSV'}}}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6233732324927571,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
